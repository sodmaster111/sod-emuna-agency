version: "3.9"

services:
  backend:
    image: sod-emuna-backend:latest
    build:
      context: .
      dockerfile: Dockerfile.backend
    env_file:
      - .env
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql+asyncpg://postgres:sodpassword@postgres:5432/sod_db}
      REDIS_URL: ${REDIS_URL:-redis://redis:6379/0}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.1:8b}
      VECTOR_DB_URL: ${VECTOR_DB_URL:-http://vector-db:6333}
      RAG_MODEL_NAME: ${RAG_MODEL_NAME:-llama3.1:8b-instruct}
      EMBEDDING_MODEL_NAME: ${EMBEDDING_MODEL_NAME:-nomic-embed-text}
      PORT: ${PORT:-8000}
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped

  worker:
    image: sod-emuna-backend:latest
    env_file:
      - .env
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql+asyncpg://postgres:sodpassword@postgres:5432/sod_db}
      REDIS_URL: ${REDIS_URL:-redis://redis:6379/0}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.1:8b}
      VECTOR_DB_URL: ${VECTOR_DB_URL:-http://vector-db:6333}
      RAG_MODEL_NAME: ${RAG_MODEL_NAME:-llama3.1:8b-instruct}
      EMBEDDING_MODEL_NAME: ${EMBEDDING_MODEL_NAME:-nomic-embed-text}
    command: celery -A app.core.celery_app.celery_app worker --loglevel=info
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped

  beat:
    image: sod-emuna-backend:latest
    env_file:
      - .env
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql+asyncpg://postgres:sodpassword@postgres:5432/sod_db}
      REDIS_URL: ${REDIS_URL:-redis://redis:6379/0}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.1:8b}
      VECTOR_DB_URL: ${VECTOR_DB_URL:-http://vector-db:6333}
      RAG_MODEL_NAME: ${RAG_MODEL_NAME:-llama3.1:8b-instruct}
      EMBEDDING_MODEL_NAME: ${EMBEDDING_MODEL_NAME:-nomic-embed-text}
    command: celery -A app.core.celery_app.celery_app beat --loglevel=info
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped

  postgres:
    image: postgres:16-alpine
    env_file:
      - .env
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-sodpassword}
      POSTGRES_DB: ${POSTGRES_DB:-sod_db}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-sod_db}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  telegram-gateway:
    build:
      context: .
    command: python -m telegram_gateway.main
    environment:
      TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN}
      BACKEND_BASE_URL: ${BACKEND_BASE_URL:-http://api:8000}
      GATEWAY_REQUEST_TIMEOUT: ${GATEWAY_REQUEST_TIMEOUT:-15}
    depends_on:
      - api
    networks:
      - sod

  # AI core services (enable when running the local brain stack)
  # ollama:
  #   image: ollama/ollama:latest
  #   volumes:
  #     - ollama_models:/root/.ollama
  #   ports:
  #     - "11434:11434"
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0
  #   # With 48 GB RAM, reserve ~16-20 GB for the Ollama runtime + 7B/8B model
  #
  # vector-db:
  #   image: qdrant/qdrant:latest
  #   ports:
  #     - "6333:6333"
  #   volumes:
  #     - qdrant_data:/qdrant/storage
  #   # Reserve ~4-8 GB RAM for Qdrant; leave headroom for backend/DB services

networks:
  sod:
    driver: bridge
